<!DOCTYPE html>
<!-- saved from url=(0031)https://mlforaudioworkshop.com/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>[“AI Heard That! ICML 2025 Workshop on Machine Learning
  for Audio
  ”] | [“Discover the harmony of AI and sound.”]</title>
<meta name="generator" content="Jekyll v3.9.5">
<meta property="og:title" content="[“Machine Learning for Audio Workshop”]">
<meta property="og:locale" content="en_US">
<meta name="description" content="[“Discover the harmony of AI and sound.”]">
<meta property="og:description" content="[“Discover the harmony of AI and sound.”]">
<link rel="canonical" href="https://MLforAudioWorkshop.github.io/">
<meta property="og:url" content="https://MLforAudioWorkshop.github.io/">
<meta property="og:site_name" content="[“Machine Learning for Audio Workshop”]">
<meta property="og:type" content="website">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="[“Machine Learning for Audio Workshop”]">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"[“Discover the harmony of AI and sound.”]","headline":"[“Machine Learning for Audio Workshop”]","name":"[“Machine Learning for Audio Workshop”]","url":"https://mlforaudio2025.com/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="./temp_files/style.css">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="https://MLforAudioWorkshop.github.io/">AI Heard That! ICML 2025 Workshop on Machine Learning
          for Audio
          </a></h1>

        

        <p>Discover the harmony of AI and sound.</p>
        
            </header>
            <section>

            <p><strong>For questions, email <a href="mailto:mlforaudioworkshop@gmail.com">mlforaudioworkshop@gmail.com</a></strong></p>
<h2 id="workshop-description">Workshop Description</h2>
<p>Machine learning research for audio applications has experienced a surge of innovation in recent years, with prominent and widely relevant advancements rapidly emerging and momentum continuing to build. There are numerous key problems within the audio research domain that continue to attract widespread attention. This ongoing relevance, alongside the success of the Machine Learning for Audio workshop at NeurIPS 2023, has inspired us to bring this workshop at ICML 2025. We believe that bringing this workshop to a wider audience will provide a good opportunity to bring together both practitioners of audio tools along with machine learning researchers interested in audio, in order to foster community, discussion, and future collaboration. In addition, with the field moving so rapidly, we believe this workshop will provide a dedicated space for the crucial ethical discussions that must be facilitated among researchers around applications of generative machine learning for audio.</p>

<p><strong>The Machine Learning for Audio workshop at ICML 2025 will cover a broad range of tasks and challenges involving audio data. These include, but are not limited to: methods of speech modeling, environmental sound generation or other forms of ambient sound, novel generative models, music generation in the form of raw audio, text-to-speech methods, denoising of speech and music, data augmentation, classification of acoustic events, transcription, source separation, and multimodal problems.</strong></p>

<p>We plan to solicit original extended abstracts (up to 4 pages) in these areas, which will be reviewed by the organizers and an additional set of reviewers. We anticipate approximately 30 accepted submissions. To avoid potential conflicts of interest, no organizer or reviewer will review a submitted paper from the same organization as the organizer or reviewer, enforced by CMT. We also plan to run a demo session alongside the poster session, where contributors will be able to present live demos of their work.</p>

<p>Our team of organizers were involved with two separate audio-related workshops at ICML 2022: the Workshop on Machine Learning for Audio Synthesis and ICML Expressive Vocalizations Workshop and Competition. We then combined our organizing committees and offered a workshop at NeurIPS 2023 entitled the Workshop on Machine Learning for Audio. This year, we have added new organizers to the team and plan to improve upon previous iterations of the workshop with a lineup of prominent in-person invited speakers, more accessible data distribution (as outlined below), and more.</p>

<h2 id="data-release">Data Release</h2>
<p>Recognizing the scarcity of free, publicly available audio data, Modulate and Hume AI will contribute several datasets in the speech domain alongside the workshop, all of large scale for their respective domains. These datasets, accessible via Google Drive, will include acted speech (professionally acted scripts), spontaneous speech (streamer content), mimicked speech (short-form emotive recordings), and mimicked non-verbal speech. The organizers hope this allows researchers from smaller research groups and academia to work with and validate findings on larger, more generalizable datasets. In previous iterations, multiple submissions utilized versions of provided data in their work, and a corresponding white paper was subsequently posted on arXiv.</p>
<p><strong>Timeline</strong></p>

<ul>
  <li>
    <p><strong>Submission deadline (main paper &amp; all supplementary material)</strong>: TBD</p>
  </li>
  <li>
    <p><strong>Accept/Reject notification date</strong>: TBD</p>
  </li>
</ul>

<h2 id="proposed-schedule">Proposed Schedule</h2>
<p>We plan for the workshop to be an 8-hour event. Below is an approximate timetable
  of the workshop schedule, subject to change. We have been careful to facilitate ample time for informal
  discussion during the coffee break, poster & demo session, and open conversation session, as well as time
  for audience participation during the panel discussion and Q&A sections following invited talks.
  </p>

  <table>
    <tr>
        <th>Time</th>
        <th>Activity</th>
        <th>Description</th>
    </tr>
    <tr>
        <td>9:00</td>
        <td>Invited Speakers 1 & 2</td>
        <td>Two 25-minute talks by invited speakers and Q&A.</td>
    </tr>
    <tr>
        <td>10:00</td>
        <td>Contributed Talks 1-3</td>
        <td>Three 15-minute contributed talks by selected submissions and Q&A.</td>
    </tr>
    <tr>
        <td>11:00</td>
        <td class="bold">Coffee Break</td>
        <td></td>
    </tr>
    <tr>
        <td>11:30</td>
        <td>Invited Speakers 3 & 4</td>
        <td>Two 25-minute talks by invited speakers and Q&A.</td>
    </tr>
    <tr>
        <td>12:30</td>
        <td class="bold">Lunch</td>
        <td></td>
    </tr>
    <tr>
        <td>1:30</td>
        <td>Poster & Demo Session</td>
        <td>Poster session alongside live demos from selected submissions.</td>
    </tr>
    <tr>
        <td>2:30</td>
        <td>Invited Speakers 5 & 6</td>
        <td>Two 25-minute talks by invited speakers and Q&A.</td>
    </tr>
    <tr>
        <td>3:30</td>
        <td>Contributed Talks 4-6</td>
        <td>Three 15-minute contributed talks by selected submissions and Q&A.</td>
    </tr>
    <tr>
        <td>4:30</td>
        <td>Panel Discussion</td>
        <td>Panel of invited speakers, where a moderator will facilitate discussion, including questions from the audience.</td>
    </tr>
    <tr>
        <td>5:00</td>
        <td>Wrap-up and Open Conversation</td>
        <td>A few minutes of closing remarks followed by informal conversation among workshop attendees.</td>
    </tr>
</table>

<p><h2 id="invited-speakers">Invited Speakers</h2>
We have curated a list of invited speakers from a wide variety of fields within the audio
domain, listed below along with brief biographies. All confirmed invited speakers will be attending in-person.</p>

<p><strong><a href="https://www.linkedin.com/in/james-betker-4a051013/">James Betker</a></strong> is a research scientist at OpenAI, where he is one of the audio leads for GPT-4o. He is also the lead author of DALL-E 3. Previously, he  created TorToiSe, a popular open source text-to-speech system. He also had a long tenure as a senior software engineer at Garmin, where he developed vehicular navigation systems. His research interests include generative models for audio and images.</p>

<p><strong><a href="https://scholar.google.com/citations?user=GRMLwjAAAAAJ&hl=fr">Jade Copet</a></strong> is a Research Engineering Manager at Meta FAIR. where she works on AudioCraft (AudioGen \& MusicGen) as well as problems related to audio compression, spoken language modeling, emotion conversion, and more. Previously, she worked as Head of AI at Linkfluence. (Unconfirmed)</p>

<p><strong><a href="https://scholar.google.com/citations?user=1H4HuCkAAAAJ&hl=en">Daniel PW Ellis</a></strong> is a research scientist at Google. From 2000-2015, he was a professor in the Electrical Engineering Department at Columbia University. In 2015, he joined Google in New York. He also runs the AUDITORY email list of over 2000 worldwide researchers in perception and cognition of sound. His research interests include speech recognition, music description, and environmental sound processing.</p>

<p><strong><a href="https://scholar.google.com/citations?user=DVCHv1kAAAAJ&hl=en&oi=ao">Albert Gu</a></strong> is an assistant professor at Carnegie Mellon University. Previously, he received his PhD from Stanford University.  He is broadly interested in theoretical and empirical aspects of deep learning. His research involves understanding and developing approaches that can be practically useful for modern large-scale machine learning models, such as his current focus on deep sequence models.  His work on state-space models, and in particular S4 and its variants, has been hugely influential in the audio community.</p>

<p><strong><a href="https://scholar.google.com/citations?user=oh4AKj0AAAAJ&hl=en">Laura Laurenti</a></strong> is a postdoctoral scholar at ETH-Zurich, where she studies the application of deep learning audio models to seismic data.  She received her PhD from La Sapienza University of Rome.  Her research includes applying deep learning to laboratory earthquakes, foundation models for seismic data, and diffusion models for seismic data.</p>

<p><strong><a href="https://scholar.google.com/citations?user=iPDwvU8AAAAJ&hl=en&oi=ao">Pratyusha Sharma</a></strong> is a PhD student in EECS at MIT, advised by Prof. Antonio Torralba and Prof. Jacob Andreas. She enjoys thinking about the interplay between language, reasoning and sequential decision making. Her research goal is to understand systems that exhibit broadly intelligent behaviors (AI systems and biological organisms) and build better AI systems. She has a broad range of speaking experience, such as invited talks at TED AI, the National Oceanic and Atmospheric Administration, and the Biennial Conference on the Biology of Marine Mammals just in the last year. Her research was also recently featured in National Geographic Magazine.</p>

<h2 id="organizers">Organizers</h2>

<p><strong><a href="https://scholar.google.com/citations?user=fHQwc60AAAAJ&hl=en&oi=ao">Alice Baird</a></strong> is a senior AI research scientist at Hume AI, NY, USA, where she works on modeling expressive human behaviors from audio and other modalities. She earned her Ph.D. at the University of Augsburg in 2022. Her work on emotion understanding from auditory, physiological, and multimodal data has been widely published in leading journals and conferences. She has co-organized several machine learning competitions, including the 2022 ICML Expressive Vocalizations Workshop and the 2023 NeurIPS Workshop on Machine Learning for Audio.</p>

<p><strong><a href="https://benanne.github.io/">Sander Dieleman</a></strong> is a research scientist at DeepMind in London, UK, where he contributed to the development of AlphaGo and WaveNet. His research focuses on generative modeling of perceptual signals at scale, including audio (speech & music) and visual data. He has co-organized multiple workshops, including the NeurIPS workshop on machine learning for creativity and design (2017-2020), the Recsys workshop on deep learning for recommender systems (2016-2018), the Machine Learning for Audio Synthesis workshop at ICML 2022, and the Workshop on Machine Learning for Audio at NeurIPS 2023.</p>

<p><strong><a href="https://chrisdonahue.com/">Chris Donahue</a></strong> is an assistant professor at Carnegie Mellon University and a research scientist at Google DeepMind. His research focuses on developing and responsibly deploying generative AI for music and creativity to unlock and augment human creative potential. His work includes improving machine learning methods for controllable generative modeling for music, audio, and sequential data, as well as deploying interactive systems that allow a broad audience—including non-musicians—to harness generative music AI through intuitive controls.</p>

<p><strong><a href="https://people.bu.edu/bkulis/">Brian Kulis</a></strong> is an associate professor at Boston University and a former Amazon Scholar who worked on Alexa. His research focuses on machine learning, particularly applications in audio problems such as detection and generation. He has won best paper awards at ICML and CVPR and has organized multiple workshops at ICCV, NeurIPS, and ICML. He has also served as an area or senior area chair at major AI conferences and has organized tutorials at ICML and ECCV.</p>

<p><strong><a href="https://davidcliu.github.io/">David Liu</a></strong> is a Ph.D. student in the Department of Computer Science at Boston University. His research focuses on deep learning for audio, with a particular emphasis on state-space models. He earned his bachelor’s degree in computer science, data science, and mathematics from the University of Wisconsin - Madison in 2023.</p>

<p><strong><a href="https://scholar.google.com/citations?hl=en&user=BzMNxvoAAAAJ">Rachel Manzelli</a></strong> is the Machine Learning Team Lead at Modulate, where she leads the development of audio generation and classification models supporting moderation teams in detecting harms in voice conversations (ToxMod) and real-time voice conversion (VoiceWear). Previously, she worked at Macro as a machine learning engineer, focusing on source separation models. She has co-organized the Machine Learning for Audio Synthesis workshop at ICML 2022 and the Workshop on Machine Learning for Audio at NeurIPS 2023. She earned her bachelor’s degree in computer engineering from Boston University in 2019, where she conducted research in structured music generation and MIR.</p>




      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://davidcliu.github.io/">David Liu</a></p>
        
        <p><small>Hosted on GitHub Pages — Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="./temp_files/scale.fix.js.download"></script>
  

</body></html>